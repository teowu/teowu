- ðŸ‘‹ Hi, Iâ€™m Teo (Timothy) Wu, Final Year PhD Candidate in Nanyang Technological University ðŸ‡¸ðŸ‡¬, [Resume](https://github.com/teowu/teowu/blob/master/2024resume.pdf), [Homepage](https://teowu.github.io)
- I am working on Large Multi-modal Models (LMMs), especially on videos and other long-form multi-modal inputs. See our [Neurips 2024 D&B, **LongVideoBench**](https://github.com/longvideobench/LongVideoBench) for the first-ever benchmark designed for long-context video-text interleaved inputs.
- I am also the contributor of **[Aria]**(https://github.com/rhymes-ai/Aria), the first native MOE LMM, with strong performance not only on visual benchmarks but also on text, coding, etc. I have been proudly contributing to its **one-of-its-kind exciting abilities** on **long-contextual inputs**, including multiple images (I do not know the limit but just try more), videos (e.g. 256 frames) and long documents (e.g. read an 9-page Arxiv paper in 63 high-res patches directly). [Try it out!](https://rhymes.ai/)
- 
Prior to working on general LMMs, I am also the creator of [Q-Future](https://github.com/Q-Future), a project that aims to utilize LMMs to boost low-level vision, visual evaluation, and related topics. Here are the two representative works:

- **Q-Align** (*Visual Scorer*): <a href="https://huggingface.co/spaces/teowu/OneScorer"><img src="https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm-dark.svg" alt="Open in Huggingface Spaces"></a>
- **Co-Instruct** (*Pro-level Low-level Vision-Language Assistant/Chatbot*): <a href="https://huggingface.co/spaces/teowu/Q-Instruct-v1"><img src="https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm-dark.svg" alt="Open in Huggingface Spaces"></a>

See my top Repos:

- - [ICML 2024, **Q-Align**](https://github.com/Q-Future/Q-Align) TL,DR: The first and yet the best visual quality and aesthetic evaluation method powered by MLLMs/LMMs.
- - [CVPR 2024, **Q-Instruct**](https://github.com/Q-Future/Q-Instruct) TL,DR: The first low-level visual instruction tuning dataset, with a model zoo of low-level-improved MLLMs!
- - [ICLR 2024 Spotlight, **Q-Bench**](https://github.com/Q-Future/Q-Bench) TL,DR: The first low-level visual benchmark for multi-modality LLMs. Focusing three tracks (perception, descrpition, and assessment).
- - [ACMMM 2023 Oral, MaxVQA/MaxWell](https://github.com/VQAssessment/MaxVQA) TL,DR: 16-dimensional VQA Dataset and Method, towards explainable VQA. Gradio demos are available in repo.
- - ðŸ¥‡ [ICCV 2023, DOVER](https://github.com/VQAssessment/DOVER) TL,DR: the SOTA NR-VQA method, can predict disentangled aesthetic and technical quality. [Colab demo](https://colab.research.google.com/github/taskswithcode/DOVER/blob/master/TWCDOVER.ipynb) available.
- - ðŸ§° [ECCV 2022, End-to-End VQA Toolbox (FAST-VQA)](https://github.com/VQAssessment/FAST-VQA-and-FasterVQA) TL, DR: An end-to-end Video Quality Assessment toolbox allowing you to develop your methods; official repo for [FAST-VQA](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660528.pdf)!
- - ðŸ¥‡ [ICME 2023 Oral, Zero-Shot BVQI](https://github.com/VQAssessment/BVQI) TL, DR: the SOTA zero-shot NR-VQA method.
- ðŸ“« Reach me by e-mail: realtimothyhwu@gmail.com, Twitter: [Twitter](https://twitter.com/HaoningTimothy)
- [Google Scholar](https://scholar.google.com.hk/citations?user=wth-VbMAAAAJ&hl=en-US)


<!---
teowu/teowu is a âœ¨ special âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->
